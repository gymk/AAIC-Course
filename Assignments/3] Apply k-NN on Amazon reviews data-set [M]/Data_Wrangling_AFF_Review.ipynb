{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQNc_cKjv4Xr"
   },
   "source": [
    "# Data Wrangling on Amazon Fine Food Review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzsW1bPxuOA0"
   },
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "NFRleFnorUVi",
    "outputId": "a7d8e7e6-6590-4dd9-acf6-70ebb5f87a02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\yuvaraja\n",
      "[nltk_data]     manikandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os # for file handling\n",
    "import sqlite3 # for database handling\n",
    "from pathlib import Path # for file management\n",
    "import pandas as pd # for handling data as frames\n",
    "import numpy as np # for matrix processing\n",
    "import csv # for CSV file handling\n",
    "#from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm # for tracking the execution progress\n",
    "import re # for regular expression over sentences for pre-processing\n",
    "from nltk.corpus import stopwords # for stopwords removal\n",
    "import string  # for punctuation mark list\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # for Bag Of Words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # for text to vector creation\n",
    "from gensim.models import Word2Vec # For Word2Vec\n",
    "#from sklearn.preprocessing import StandardScaler # for Column Standardization\n",
    "from sklearn.preprocessing import MinMaxScaler # for Row Standardization\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD # for reducing Bow/TFIDF dimension\n",
    "\n",
    "import scipy.sparse # for storing sparse matrix\n",
    "\n",
    "import pickle # for storing review polarities\n",
    "\n",
    "import nltk # for pre-processing text data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from prettytable import PrettyTable # for pretty table\n",
    "from matplotlib import pyplot as plt # for pie char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the outputs generated by this notebook will be placed in a separate folder\n",
    "output_dir = 'Output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "def getOutputFileNamePath(file_name):\n",
    "    return str(Path.cwd() / output_dir / file_name)\n",
    "\n",
    "def saveListToFile(file_name, my_list):\n",
    "    '''\n",
    "    Helper function to save the given list into a file\n",
    "    Each item is considered as a string and stored in a separate line\n",
    "    '''\n",
    "    with open(getOutputFileNamePath(file_name), 'w') as f:\n",
    "        for item in my_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "def storeSet(w_set, file_name):\n",
    "    with open(getOutputFileNamePath(file_name), 'w', encoding=\"utf-8\") as csv_file:\n",
    "        for w in w_set:\n",
    "            csv_file.write(str(w))\n",
    "            csv_file.write('\\n')\n",
    "            \n",
    "            \n",
    "# https://stats.stackexchange.com/questions/340933/truncatedsvd-always-reduces-dataset-to-1d\n",
    "def GetRandomizedSVD(**kwargs):\n",
    "    '''\n",
    "    Common function wtih fixed SVD configuration for TruncatedSVD\n",
    "    '''\n",
    "    return TruncatedSVD(algorithm='randomized',\n",
    "                        random_state=42,\n",
    "                        **kwargs)\n",
    "\n",
    "def GetOptimalDimension(X,req_variance=0.95):\n",
    "    '''\n",
    "    Function to get dimension on SVD having 95% variance\n",
    "    Input:\n",
    "        X: Numpy Array of the data\n",
    "        req_variance - Require varianced (default 0.95)\n",
    "    Output:\n",
    "        Max dimensions after SVD that is needed to have the requested variance\n",
    "    '''\n",
    "\n",
    "    max_svd_components = X.get_shape()[1] - 1\n",
    "    print('GetOptimalDimension -> Actual: ', X.get_shape()[1], ' Selected: ', max_svd_components)\n",
    "    svd = GetRandomizedSVD(n_components=max_svd_components)\n",
    "    svd.fit(X)\n",
    "    cumsum = np.cumsum(svd.explained_variance_ratio_)\n",
    "    max_svd_components = np.argmax(cumsum >= req_variance) + 1\n",
    "    print('GetOptimalDimension -> Final SVD Component Size: ', max_svd_components)\n",
    "    #print('Cum Var: ', cumsum)\n",
    "    \n",
    "    return max_svd_components\n",
    "    \n",
    "    '''\n",
    "    print('Final SVD Component Size: ', max_svd_components, 'Cum Var: ', cumsum)\n",
    "\n",
    "    svd = get_svd(n_components=max_svd_components)\n",
    "    return svd.fit_transform(X)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z5p4AvmYuSfR"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "pGhcctjOrVd1",
    "outputId": "bdabc96f-11e5-425a-a208-385916d2fb7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using sqlite read data from the database\n",
    "#con = sqlite3.connect('/content/drive/My Drive/Colab Notebooks/AFF-Review/database.sqlite')\n",
    "db_path = Path.cwd() / 'database.sqlite'\n",
    "con = sqlite3.connect(str(db_path))\n",
    "\n",
    "# Get reviews which do not have score as 3\n",
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", con)\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMJfYN6zs2oK",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Insights about Data (Highlevel Statistics)\n",
    "\n",
    "Let me try to understand the dataset that is given to me (Basically 'Understanding the Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "oyMOh4EEs11i",
    "outputId": "a60cae32-7c80-4767-9d52-0e02720f5e2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>525814.000000</td>\n",
       "      <td>525814.000000</td>\n",
       "      <td>525814.000000</td>\n",
       "      <td>525814.000000</td>\n",
       "      <td>5.258140e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>284599.060038</td>\n",
       "      <td>1.747293</td>\n",
       "      <td>2.209544</td>\n",
       "      <td>4.279148</td>\n",
       "      <td>1.295943e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>163984.038077</td>\n",
       "      <td>7.575819</td>\n",
       "      <td>8.195329</td>\n",
       "      <td>1.316725</td>\n",
       "      <td>4.828129e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.393408e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>142730.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.270598e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>284989.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.310861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>426446.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.332634e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>568454.000000</td>\n",
       "      <td>866.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.351210e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
       "count  525814.000000         525814.000000           525814.000000   \n",
       "mean   284599.060038              1.747293                2.209544   \n",
       "std    163984.038077              7.575819                8.195329   \n",
       "min         1.000000              0.000000                0.000000   \n",
       "25%    142730.250000              0.000000                0.000000   \n",
       "50%    284989.500000              0.000000                1.000000   \n",
       "75%    426446.750000              2.000000                2.000000   \n",
       "max    568454.000000            866.000000              878.000000   \n",
       "\n",
       "               Score          Time  \n",
       "count  525814.000000  5.258140e+05  \n",
       "mean        4.279148  1.295943e+09  \n",
       "std         1.316725  4.828129e+07  \n",
       "min         1.000000  9.393408e+08  \n",
       "25%         4.000000  1.270598e+09  \n",
       "50%         5.000000  1.310861e+09  \n",
       "75%         5.000000  1.332634e+09  \n",
       "max         5.000000  1.351210e+09  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aA98xN2syNkZ"
   },
   "source": [
    "## Features/ Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "P_1bF4ias-c-",
    "outputId": "a0fb4e18-5377-4143-f86c-e360b85781f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
      "      dtype='object')\n",
      "Id                         int64\n",
      "ProductId                 object\n",
      "UserId                    object\n",
      "ProfileName               object\n",
      "HelpfulnessNumerator       int64\n",
      "HelpfulnessDenominator     int64\n",
      "Score                      int64\n",
      "Time                       int64\n",
      "Summary                   object\n",
      "Text                      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(filtered_data.columns)\n",
    "print(filtered_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vCWWfniyVSQ"
   },
   "source": [
    "### Observation\n",
    "- Totally 10 features given\n",
    "- No labels given\n",
    "- From Kaggle below information I have obtained about teach feature\n",
    "  - https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "- Id\n",
    "  - Row Id\n",
    "- ProductId\n",
    "  - Unique identifier for the product\n",
    "- UserId\n",
    "  - Unqiue identifier for the user\n",
    "- ProfileName\n",
    "  - Profile name of the user\n",
    "- HelpfulnessNumerator\n",
    "  - Number of users who found the review helpful\n",
    "- HelpfulnessDenominator\n",
    "  - Number of users who indicated whether they found the review helpful\n",
    "- Score\n",
    "  - Rating between 1 and 5\n",
    "- Time\n",
    "  - Timestamp for the review\n",
    "- Summary\n",
    "  - Brief summary of the review\n",
    "- Text\n",
    "  - Text of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80EtBiCmz6in"
   },
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Since it is a text corpus, before feature creation, data neet to be cleaned.\n",
    "\n",
    "I have executed this stage in two steps\n",
    "\n",
    "1. First analyse the give data for abnormality\n",
    "2. Execute the cleaning process based on previous step observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPu-A1qOz9Gw",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ihRJz7xHtDB7",
    "outputId": "635a68ba-a97d-4f27-d307-7f5df932d89e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Id\n",
    "u = filtered_data.Id.value_counts()\n",
    "u.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fxAoM0yTvScQ",
    "outputId": "7958658f-a5b4-4e2b-930a-a290dfb456b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72005"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ProductId\n",
    "len(filtered_data.ProductId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EVCROWUnvdEa",
    "outputId": "bb7a4794-18c0-4a07-f750-49acffc8f850"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243414"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UserId\n",
    "len(filtered_data.UserId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "taWumONlw2y8",
    "outputId": "9f2fdc41-bc1b-4b21-e0fe-ec204d9522fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 866 222\n"
     ]
    }
   ],
   "source": [
    "# HelpfulnessNumerator\n",
    "print(filtered_data.HelpfulnessNumerator.min(),\n",
    "      filtered_data.HelpfulnessNumerator.max(),\n",
    "      len(filtered_data.HelpfulnessNumerator.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1dlsXJ18xOTE",
    "outputId": "2b53928b-d81b-43f6-8f50-7a5a33846516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 878 227\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41159</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59301</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId              ProfileName  \\\n",
       "41159  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "59301  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "\n",
       "       HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "41159                     3                       2      4  1212883200   \n",
       "59301                     3                       1      5  1224892800   \n",
       "\n",
       "                                            Summary  \\\n",
       "41159  Pure cocoa taste with crunchy almonds inside   \n",
       "59301             Bought This for My Son at College   \n",
       "\n",
       "                                                    Text  \n",
       "41159  It was almost a 'love at first bite' - the per...  \n",
       "59301  My son loves spaghetti so I didn't hesitate or...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HelpfulnessDenominator\n",
    "print(filtered_data.HelpfulnessDenominator.min(),\n",
    "      filtered_data.HelpfulnessDenominator.max(),\n",
    "      len(filtered_data.HelpfulnessDenominator.unique()))\n",
    "\n",
    "# As per feature details, Denominator should be greater than Numerator\n",
    "# Lets check whether the data follows that description\n",
    "filtered_data[(filtered_data.HelpfulnessDenominator < filtered_data.HelpfulnessNumerator)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0860X5gkx3-W",
    "outputId": "9e764c50-00a3-49f5-8be0-8ac17845bf32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1 4 2]\n",
      "5    363122\n",
      "4     80655\n",
      "1     52268\n",
      "2     29769\n",
      "Name: Score, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Score\n",
    "print(filtered_data.Score.unique())\n",
    "print(filtered_data.Score.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tGA6Zfra1cLc",
    "outputId": "3f23e34d-3d42-444f-cd2c-5b7bc43983b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3157\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>346055</th>\n",
       "      <td>374359</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>944438400</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417859</th>\n",
       "      <td>451878</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>944438400</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212472</th>\n",
       "      <td>230285</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>944438400</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346116</th>\n",
       "      <td>374422</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A1048CYU0OV4O8</td>\n",
       "      <td>Judy L. Eans</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>947376000</td>\n",
       "      <td>GREAT</td>\n",
       "      <td>THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417927</th>\n",
       "      <td>451949</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1048CYU0OV4O8</td>\n",
       "      <td>Judy L. Eans</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>947376000</td>\n",
       "      <td>GREAT</td>\n",
       "      <td>THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId      ProfileName  \\\n",
       "346055  374359  B00004CI84  A344SMIA5JECGM  Vincent P. Ross   \n",
       "417859  451878  B00004CXX9  A344SMIA5JECGM  Vincent P. Ross   \n",
       "212472  230285  B00004RYGX  A344SMIA5JECGM  Vincent P. Ross   \n",
       "346116  374422  B00004CI84  A1048CYU0OV4O8     Judy L. Eans   \n",
       "417927  451949  B00004CXX9  A1048CYU0OV4O8     Judy L. Eans   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "346055                     1                       2      5  944438400   \n",
       "417859                     1                       2      5  944438400   \n",
       "212472                     1                       2      5  944438400   \n",
       "346116                     2                       2      5  947376000   \n",
       "417927                     2                       2      5  947376000   \n",
       "\n",
       "                        Summary  \\\n",
       "346055  A modern day fairy tale   \n",
       "417859  A modern day fairy tale   \n",
       "212472  A modern day fairy tale   \n",
       "346116                    GREAT   \n",
       "417927                    GREAT   \n",
       "\n",
       "                                                     Text  \n",
       "346055  A twist of rumplestiskin captured on film, sta...  \n",
       "417859  A twist of rumplestiskin captured on film, sta...  \n",
       "212472  A twist of rumplestiskin captured on film, sta...  \n",
       "346116  THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...  \n",
       "417927  THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time\n",
    "print(len(filtered_data.Time.unique()))\n",
    "#filtered_data['Time'].value_counts()\n",
    "\n",
    "# Check whether any entry with same time for more than one product\n",
    "# which is practically not possible\n",
    "userid_group = filtered_data.groupby('UserId')\n",
    "#g = userid_group.groups\n",
    "#g.values()\n",
    "\n",
    "fil_val = userid_group.filter(lambda x:len(x)>1).sort_values('Time')\n",
    "fil_val.to_csv(getOutputFileNamePath('user_id_grouped.csv'))\n",
    "fil_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invalid Review check / Analysis - on Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWIpdXLdBwsX"
   },
   "outputs": [],
   "source": [
    "def getEntriesHavingWords(df, col_to_search, regex_list):\n",
    "    '''\n",
    "    Function to search for given list of regex expressions over the requested column\n",
    "    '''\n",
    "    indices = []\n",
    "    counts = []\n",
    "    for regex_val in regex_list:\n",
    "        l = df[df[col_to_search].str.contains(regex_val,regex=True)].index.tolist()\n",
    "        counts.append(len(l))\n",
    "        indices = indices + l\n",
    "    return indices, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ufS9B0mdPm_O",
    "outputId": "0c373cfc-4995-49f5-a31f-de6ebe10fdd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of entries having 'Tim Burton' is 36\n",
      "No. of entries having '\\b[bB]ook\\b' is 58\n",
      "No. of entries having '\\b[fF]ilm\\b' is 17\n",
      "Total suspicious entries :  111\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212391</th>\n",
       "      <td>230200</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A2H1WNB30JNAWU</td>\n",
       "      <td>Jack D. Lowry</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1345939200</td>\n",
       "      <td>One of Tim Burton's best movies</td>\n",
       "      <td>Beetlejuice one of Tim Burton's best movies he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212417</th>\n",
       "      <td>230228</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>ASJ54MITON1NO</td>\n",
       "      <td>Dr. Feelgood \"Dr. Feelgood\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1254873600</td>\n",
       "      <td>1988 Tim Burton film, dvd.</td>\n",
       "      <td>Tim Burton's unique vision, which is almost ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212423</th>\n",
       "      <td>230234</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A1FMJJKSVQDDQ</td>\n",
       "      <td>Eric S. Kim</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1224460800</td>\n",
       "      <td>Tim Burton's imagination goes wild!</td>\n",
       "      <td>I'd have to say that this isn't really a top f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212439</th>\n",
       "      <td>230250</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A1TW9ZGRDQQZ2Y</td>\n",
       "      <td>Monkdude</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1109635200</td>\n",
       "      <td>One of Tim Burton's earliest and best films</td>\n",
       "      <td>Edited this review on 10/28/08.&lt;br /&gt;&lt;br /&gt;Eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212444</th>\n",
       "      <td>230255</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A1JZV9MCT6KOX4</td>\n",
       "      <td>C. Eallonardo \"Kali's Copilot\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1037923200</td>\n",
       "      <td>Good Tim Burton Flick</td>\n",
       "      <td>I like Tim Burton movies in general.  But this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                     ProfileName  \\\n",
       "212391  230200  B00004RYGX  A2H1WNB30JNAWU                   Jack D. Lowry   \n",
       "212417  230228  B00004RYGX   ASJ54MITON1NO     Dr. Feelgood \"Dr. Feelgood\"   \n",
       "212423  230234  B00004RYGX   A1FMJJKSVQDDQ                     Eric S. Kim   \n",
       "212439  230250  B00004RYGX  A1TW9ZGRDQQZ2Y                        Monkdude   \n",
       "212444  230255  B00004RYGX  A1JZV9MCT6KOX4  C. Eallonardo \"Kali's Copilot\"   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "212391                     0                       0      5  1345939200   \n",
       "212417                     0                       0      5  1254873600   \n",
       "212423                     0                       0      4  1224460800   \n",
       "212439                     0                       0      5  1109635200   \n",
       "212444                     0                       0      5  1037923200   \n",
       "\n",
       "                                            Summary  \\\n",
       "212391              One of Tim Burton's best movies   \n",
       "212417                   1988 Tim Burton film, dvd.   \n",
       "212423          Tim Burton's imagination goes wild!   \n",
       "212439  One of Tim Burton's earliest and best films   \n",
       "212444                        Good Tim Burton Flick   \n",
       "\n",
       "                                                     Text  \n",
       "212391  Beetlejuice one of Tim Burton's best movies he...  \n",
       "212417  Tim Burton's unique vision, which is almost ca...  \n",
       "212423  I'd have to say that this isn't really a top f...  \n",
       "212439  Edited this review on 10/28/08.<br /><br />Eve...  \n",
       "212444  I like Tim Burton movies in general.  But this...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = ['Tim Burton', r'\\b[bB]ook\\b', r'\\b[fF]ilm\\b']\n",
    "suspicious_indices, counts = getEntriesHavingWords(filtered_data,\n",
    "                                       'Summary',\n",
    "                                       text_list)\n",
    "\n",
    "for i in range(len(counts)):\n",
    "    print(\"No. of entries having '{0}' is {1}\".format(text_list[i], counts[i]))\n",
    "\n",
    "print('Total suspicious entries : ', len(suspicious_indices))\n",
    "save_data = filtered_data.iloc[suspicious_indices]\n",
    "save_data.to_csv(getOutputFileNamePath('non_related_review_entries.csv'))\n",
    "save_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8yGboODzZP5f"
   },
   "source": [
    "### Invalid Entry check /  analysis - on review text\n",
    "\n",
    "Since checking this process takes long time,\n",
    "after this check,\n",
    "I have disabled this code to avoid huge delay in  pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWords(df, col_name):\n",
    "    words = [w for index,row in df[col_name].items() for w in row.split()]\n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab_type": "text",
    "id": "CxZjf9qmQNI0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 479 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "summary_words = getUniqueWords(filtered_data, 'Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab_type": "text",
    "id": "kW8fWkZvQOV6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_words = getUniqueWords(filtered_data, 'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab_type": "text",
    "id": "yK4Eb0QAQP0q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in Summary:  98264\n",
      "Total unique words in Review Text:  553862\n"
     ]
    }
   ],
   "source": [
    "print('Total unique words in Summary: ', len(summary_words))\n",
    "print('Total unique words in Review Text: ', len(text_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab_type": "text",
    "id": "N6wgUUmkQR72"
   },
   "outputs": [],
   "source": [
    "storeSet(summary_words, 'summary_words.csv')\n",
    "storeSet(text_words, 'text_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab_type": "text",
    "id": "1qCAu8seQUON"
   },
   "outputs": [],
   "source": [
    "invalidChars = set(string.punctuation.replace(\"_\", \"\"))\n",
    "\n",
    "def containsAny(word, char_list):\n",
    "    '''\n",
    "    If any of the character in char_list found in 'word' will return True\n",
    "    Otherwise returns False\n",
    "    '''\n",
    "    for c in char_list:\n",
    "        if c in word:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def containsAll(word, char_list):\n",
    "    '''\n",
    "    If all of the characters in char_list found in 'word' will return True\n",
    "    Otherwise returns False\n",
    "    '''\n",
    "    for c in char_list:\n",
    "        if c not in word:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab_type": "text",
    "id": "wQjUh2JbQWF6"
   },
   "outputs": [],
   "source": [
    "def getWordsHavingSpecialChar(df, col_name):\n",
    "    '''\n",
    "    Function to get list of words having special characters in the requested column\n",
    "    '''\n",
    "    words = []\n",
    "    count  = 0\n",
    "    for row in tqdm(df[col_name],ascii=True):\n",
    "        w_c_l = []\n",
    "        for w in row.split():\n",
    "            if containsAny(w, invalidChars):\n",
    "                words.append(w)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab_type": "text",
    "id": "vgcHqRYHQW9u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|######################################################################| 525814/525814 [00:03<00:00, 140180.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "summary_invalid_words = getWordsHavingSpecialChar(filtered_data, 'Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab_type": "text",
    "id": "9BURWhldQYtu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|########################################################################| 525814/525814 [00:54<00:00, 9734.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_invalid_words = getWordsHavingSpecialChar(filtered_data, 'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab_type": "text",
    "id": "_mD4qD0jQbzb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique (invalid) words in Summary:  345554\n",
      "Total unique (invalid) words in Review Text:  6728685\n"
     ]
    }
   ],
   "source": [
    "print('Total unique (invalid) words in Summary: ', len(summary_invalid_words))\n",
    "print('Total unique (invalid) words in Review Text: ', len(text_invalid_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab_type": "text",
    "id": "CA7nbf9-QdT6"
   },
   "outputs": [],
   "source": [
    "storeSet(summary_invalid_words, 'summary_invalid_words.csv')\n",
    "storeSet(text_invalid_words, 'text_invalid_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tn1HrTOpN-R9"
   },
   "source": [
    "## Observation Summary\n",
    "\n",
    "- Id\n",
    "    - No Id repeation\n",
    "- ProductId\n",
    "    - 72005 Products\n",
    "- UserId\n",
    "    - 243414 Users\n",
    "- HelpfulnessNumerator\n",
    "    - value ranges from 0 to 808\n",
    "    - 222 unique entries\n",
    "- HelpfulnessDenominator\n",
    "    - value ranges from 0 to 878\n",
    "    - 227 unique entries\n",
    "    - **2 invalid entries found**\n",
    "      - Denominator is greater than Numerator\n",
    "- Score\n",
    "    - Scores range from 1 to 5 only\n",
    "    - No invalid entries found\n",
    "    - **No equal amount of data points for each score**\n",
    "      - We have an __IMBALANCED__ dataset\n",
    "\n",
    "- Entries with book/Book words found in text reviews\n",
    "- Entries with film/Film words found in text reviews\n",
    "- There are duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXgum8ANWRIU"
   },
   "source": [
    "## Cleaning\n",
    "\n",
    "Actual cleaning process I am doing here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDkUqrnvnai9"
   },
   "source": [
    "### Convert Score to Positive/Negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L6lz9vKcnh0a",
    "outputId": "6f293e05-eda0-4fd6-ba3c-1fee6f77e4f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "def ScoreToReviewType(score):\n",
    "    if score < 3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "filtered_data.Score = filtered_data.Score.map(ScoreToReviewType)\n",
    "print(filtered_data.Score.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0RWUIiqXGNR"
   },
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bq5iOkxlWdeZ"
   },
   "outputs": [],
   "source": [
    "# Sort the data based on ProductID in ascending order so that we can keep only one kind of product review\n",
    "sorted_data = filtered_data.sort_values('Time',axis=0, ascending=True, inplace=False, na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328732, 10)\n"
     ]
    }
   ],
   "source": [
    "# keep first entry, drop remaining duplicate entries\n",
    "final_data = sorted_data.drop_duplicates(subset={'UserId','Time'},keep='first',inplace=False)\n",
    "print(final_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-checking for NaN removal\n",
    "final_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqFfyoYXYo-A"
   },
   "source": [
    "### Remove Invalid Helpfull Score entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eTHCvfhMYzKZ",
    "outputId": "1a55244b-a5e2-48ca-ad0f-20e9b5216eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328731, 10)\n"
     ]
    }
   ],
   "source": [
    "# Drop data having invalid helpful score entries\n",
    "# that is removing entries which has denominator greater than numerator, which is practically impossible\n",
    "final_data = final_data[final_data.HelpfulnessNumerator <= final_data.HelpfulnessDenominator]\n",
    "print(final_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DFnSdWgAZMWc",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Remove Invalid Summary Entries\n",
    "\n",
    "- Remove actual film reviews\n",
    "- Tim Burton (found by filtering film words and looking into data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328674, 10)\n"
     ]
    }
   ],
   "source": [
    "# Remove reviews unrelated to foods\n",
    "\n",
    "text_list = ['Tim Burton', r'\\b[bB]ook\\b', r'\\b[fF]ilm\\b']\n",
    "suspicious_indices, counts = getEntriesHavingWords(final_data,\n",
    "                                       'Summary',\n",
    "                                       text_list)\n",
    "\n",
    "final_data = final_data.drop(suspicious_indices)\n",
    "print(final_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBH32iW2bwhG"
   },
   "source": [
    "### Remove Invalid Text (Review) Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bIy83xN9QHz-",
    "outputId": "73681697-aa9b-4a86-b931-53aac72c18be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328622, 10)\n"
     ]
    }
   ],
   "source": [
    "# Remove film reviews found in Review Text\n",
    "final_data = final_data[~final_data.Text.str.contains('Tim Burton')]\n",
    "print(final_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mUoUzH5rb4L5",
    "outputId": "612c8fab-b82f-4a1e-f1f2-0da21081d542"
   },
   "outputs": [],
   "source": [
    "def removeHtmlTags(sentence):\n",
    "    '''\n",
    "    function to remove HTML tags in the given sentence\n",
    "    '''\n",
    "    reg_exp = re.compile('<.*?>', )\n",
    "    cleaned_text = re.sub(reg_exp, ' ', sentence)\n",
    "    return cleaned_text\n",
    "\n",
    "def removePunctuations(sentence):\n",
    "    '''\n",
    "    function to remove punctuations in the given sentence\n",
    "    '''\n",
    "    cleaned_sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned_sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned_sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "# s = 'Hi I am <pr> test </pr> testing'\n",
    "# removeHtmlTags(s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9yr_vnPeggN"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) # get stop words for English\n",
    "#print(stop)\n",
    "snow_stem = nltk.stem.SnowballStemmer('english') # get Stemmer for English\n",
    "#print(snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290598
    },
    "colab_type": "code",
    "id": "xttCEa7mfWLe",
    "outputId": "8bdb678f-a03e-41f7-abf8-db2a00e3b8b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################| 328622/328622 [06:31<00:00, 840.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating final dataset set using/following steps\n",
    "\n",
    "# 1. Removing HTML tags that are found in my above analysis\n",
    "# 2. Removing punctuations, which has no meaning as a word\n",
    "# 3. Stemming words based on English vocabulary set from NLTK\n",
    "# 4. Creating a seperate list for both positive and negative cases, having only those words \n",
    "\n",
    "all_positive_words = []\n",
    "all_negative_words = []\n",
    "final_review_texts = []\n",
    "df_index = 0 # for tracking the observations\n",
    "\n",
    "for sent in tqdm(final_data['Text'].values,ascii=True):\n",
    "    #print('{0} ==> '.format(df_index), sent)\n",
    "    sent = removeHtmlTags(sent) # remove HTML tags first\n",
    "    #print('{0} ==> '.format(df_index), sent)\n",
    "\n",
    "    filtered_words = []\n",
    "    for w in sent.split():\n",
    "        #print(removePunctuations(w))\n",
    "        for cleaned_word in removePunctuations(w).split():\n",
    "            if ((cleaned_word.isalpha()) & (len(cleaned_word) > 2)):\n",
    "                cleaned_word = cleaned_word.lower()\n",
    "                #print(cleaned_word)\n",
    "                if (cleaned_word not in stop_words):\n",
    "                    s = (snow_stem.stem(cleaned_word)).encode('utf8')\n",
    "                    filtered_words.append(s)\n",
    "                    if ((final_data['Score'].values)[df_index] == 1):\n",
    "                        all_positive_words.append(s)\n",
    "                    else:\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    filtered_sent = b\" \".join(filtered_words)\n",
    "    #print(filtered_words, filtered_sent)\n",
    "    final_review_texts.append(filtered_sent)\n",
    "\n",
    "    df_index += 1\n",
    "    #if df_index > 10:\n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328622"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_review_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "FQD4tQw017Jj",
    "outputId": "757e85c3-0a94-4f7f-9793-5d4e29b7cec0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138683</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>b'rememb see show air televis year ago child s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417839</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>b'beetlejuic well written movi everyth excel a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417838</th>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>b'beetlejuic excel funni movi keaton hilari wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417927</th>\n",
       "      <td>451949</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1048CYU0OV4O8</td>\n",
       "      <td>Judy L. Eans</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>947376000</td>\n",
       "      <td>GREAT</td>\n",
       "      <td>THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...</td>\n",
       "      <td>b'one movi movi collect fill comedi action wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417847</th>\n",
       "      <td>451864</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1B2IZU1JLZA6</td>\n",
       "      <td>Wes</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>948240000</td>\n",
       "      <td>WARNING: CLAMSHELL EDITION IS EDITED TV VERSION</td>\n",
       "      <td>I, myself always enjoyed this movie, it's very...</td>\n",
       "      <td>b'alway enjoy movi funni entertain didnt hesit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId               ProfileName  \\\n",
       "138683  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "417839  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "417838  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "417927  451949  B00004CXX9  A1048CYU0OV4O8              Judy L. Eans   \n",
       "417847  451864  B00004CXX9   A1B2IZU1JLZA6                       Wes   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "138683                     2                       2      1  940809600   \n",
       "417839                     0                       0      1  944092800   \n",
       "417838                     0                       0      1  946857600   \n",
       "417927                     2                       2      1  947376000   \n",
       "417847                    19                      23      0  948240000   \n",
       "\n",
       "                                                  Summary  \\\n",
       "138683  This whole series is great way to spend time w...   \n",
       "417839                               Entertainingl Funny!   \n",
       "417838                                         FANTASTIC!   \n",
       "417927                                              GREAT   \n",
       "417847    WARNING: CLAMSHELL EDITION IS EDITED TV VERSION   \n",
       "\n",
       "                                                     Text  \\\n",
       "138683  I can remember seeing the show when it aired o...   \n",
       "417839  Beetlejuice is a well written movie ..... ever...   \n",
       "417838  Beetlejuice is an excellent and funny movie. K...   \n",
       "417927  THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...   \n",
       "417847  I, myself always enjoyed this movie, it's very...   \n",
       "\n",
       "                                              CleanedText  \n",
       "138683  b'rememb see show air televis year ago child s...  \n",
       "417839  b'beetlejuic well written movi everyth excel a...  \n",
       "417838  b'beetlejuic excel funni movi keaton hilari wa...  \n",
       "417927  b'one movi movi collect fill comedi action wha...  \n",
       "417847  b'alway enjoy movi funni entertain didnt hesit...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add cleaned text as a seperate column (feature) into our final data dataframe\n",
    "# It will easy me in handling the cleaned data\n",
    "\n",
    "final_data['CleanedText'] = final_review_texts\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data based on Time\n",
    "sorted_data = final_data.sort_values('Time',axis=0, ascending=True, inplace=False, na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328622\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138683</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>b'rememb see show air televis year ago child s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417839</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>b'beetlejuic well written movi everyth excel a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417838</th>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>b'beetlejuic excel funni movi keaton hilari wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417927</th>\n",
       "      <td>451949</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1048CYU0OV4O8</td>\n",
       "      <td>Judy L. Eans</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>947376000</td>\n",
       "      <td>GREAT</td>\n",
       "      <td>THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...</td>\n",
       "      <td>b'one movi movi collect fill comedi action wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417847</th>\n",
       "      <td>451864</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1B2IZU1JLZA6</td>\n",
       "      <td>Wes</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>948240000</td>\n",
       "      <td>WARNING: CLAMSHELL EDITION IS EDITED TV VERSION</td>\n",
       "      <td>I, myself always enjoyed this movie, it's very...</td>\n",
       "      <td>b'alway enjoy movi funni entertain didnt hesit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId               ProfileName  \\\n",
       "138683  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "417839  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "417838  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "417927  451949  B00004CXX9  A1048CYU0OV4O8              Judy L. Eans   \n",
       "417847  451864  B00004CXX9   A1B2IZU1JLZA6                       Wes   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "138683                     2                       2      1  940809600   \n",
       "417839                     0                       0      1  944092800   \n",
       "417838                     0                       0      1  946857600   \n",
       "417927                     2                       2      1  947376000   \n",
       "417847                    19                      23      0  948240000   \n",
       "\n",
       "                                                  Summary  \\\n",
       "138683  This whole series is great way to spend time w...   \n",
       "417839                               Entertainingl Funny!   \n",
       "417838                                         FANTASTIC!   \n",
       "417927                                              GREAT   \n",
       "417847    WARNING: CLAMSHELL EDITION IS EDITED TV VERSION   \n",
       "\n",
       "                                                     Text  \\\n",
       "138683  I can remember seeing the show when it aired o...   \n",
       "417839  Beetlejuice is a well written movie ..... ever...   \n",
       "417838  Beetlejuice is an excellent and funny movie. K...   \n",
       "417927  THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...   \n",
       "417847  I, myself always enjoyed this movie, it's very...   \n",
       "\n",
       "                                              CleanedText  \n",
       "138683  b'rememb see show air televis year ago child s...  \n",
       "417839  b'beetlejuic well written movi everyth excel a...  \n",
       "417838  b'beetlejuic excel funni movi keaton hilari wa...  \n",
       "417927  b'one movi movi collect fill comedi action wha...  \n",
       "417847  b'alway enjoy movi funni entertain didnt hesit...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sorted_data))\n",
    "sorted_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9M4-jCdo2KEl"
   },
   "outputs": [],
   "source": [
    "# store final data into new database\n",
    "conn = sqlite3.connect(getOutputFileNamePath('cleaned.sqlite'))\n",
    "c = conn.cursor()\n",
    "conn.text_factory = str\n",
    "sorted_data.to_sql('Reviews', conn, schema=None, if_exists='replace', \n",
    "                  index=True, index_label=None, dtype=None)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store review polarities in a seperate file\n",
    "with open(getOutputFileNamePath('positive_words.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_positive_words, f)\n",
    "    \n",
    "with open(getOutputFileNamePath('negative_words.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_negative_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWords_2(df, col_name):\n",
    "    words = [w for index,row in df[col_name].items() for w in row.decode('utf-8').split()]\n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in Cleaned Text:  68864\n",
      "Wall time: 3.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# How many unique words do we have in cleaned text?\n",
    "text_words_cleaned = getUniqueWords_2(sorted_data, 'CleanedText')\n",
    "print('Total unique words in Cleaned Text: ', len(text_words_cleaned))\n",
    "storeSet(text_words_cleaned, 'cleanded_text_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeSet(text_words_cleaned, 'cleanded_text_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsHavingSpecialChar_2(df, col_name):\n",
    "    '''\n",
    "    Function to get list of words having special characters in the requested column\n",
    "    '''\n",
    "    words = []\n",
    "    count  = 0\n",
    "    for row in tqdm(df[col_name],ascii=True):\n",
    "        for w in row.decode('utf-8').split():\n",
    "            if containsAny(w, invalidChars):\n",
    "                words.append(w)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#######################################################################| 328622/328622 [00:17<00:00, 18492.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique (invalid) words in Cleaned Text:  0\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Do we have any invalid words still in cleaned text?\n",
    "text_invalid_words_cleanded = getWordsHavingSpecialChar_2(sorted_data, 'CleanedText')\n",
    "print('Total unique (invalid) words in Cleaned Text: ', len(text_invalid_words_cleanded))\n",
    "storeSet(text_invalid_words_cleanded, 'cleanded_invalid_text_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Preparation (splitting)\n",
    "\n",
    "- We have approximately 328K reviews sorted according to its entry timestamp\n",
    "- Due to memory limitation, I will be taking first 100K Reviews for Study\n",
    "- Selected dataset will be splitted into:\n",
    "    - First 60% as Training Data set\n",
    "    - Next 20% as Cross Validation Data set\n",
    "    - Remaining 20% as Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 90000, CV 30000, Test 30000, Total 150000\n"
     ]
    }
   ],
   "source": [
    "total_entries = 150 * 1000 # 150K\n",
    "\n",
    "n_train_entries = int(total_entries * 0.60)\n",
    "n_cv_entries = int(total_entries * 0.20)\n",
    "n_test_entries = int(total_entries * 0.20)\n",
    "\n",
    "print('Train {0}, CV {1}, Test {2}, Total {3}'.format(n_train_entries, n_cv_entries, n_test_entries, total_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(getOutputFileNamePath('cleaned.sqlite'))\n",
    "df = pd.read_sql_query(\"\"\"SELECT * from Reviews\"\"\", con)\n",
    "df.head()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138683</td>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>b'rememb see show air televis year ago child s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>417839</td>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>b'beetlejuic well written movi everyth excel a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>417838</td>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>b'beetlejuic excel funni movi keaton hilari wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417927</td>\n",
       "      <td>451949</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1048CYU0OV4O8</td>\n",
       "      <td>Judy L. Eans</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>947376000</td>\n",
       "      <td>GREAT</td>\n",
       "      <td>THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...</td>\n",
       "      <td>b'one movi movi collect fill comedi action wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>417847</td>\n",
       "      <td>451864</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1B2IZU1JLZA6</td>\n",
       "      <td>Wes</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>948240000</td>\n",
       "      <td>WARNING: CLAMSHELL EDITION IS EDITED TV VERSION</td>\n",
       "      <td>I, myself always enjoyed this movie, it's very...</td>\n",
       "      <td>b'alway enjoy movi funni entertain didnt hesit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index      Id   ProductId          UserId               ProfileName  \\\n",
       "0  138683  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "1  417839  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "2  417838  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "3  417927  451949  B00004CXX9  A1048CYU0OV4O8              Judy L. Eans   \n",
       "4  417847  451864  B00004CXX9   A1B2IZU1JLZA6                       Wes   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "0                     2                       2      1  940809600   \n",
       "1                     0                       0      1  944092800   \n",
       "2                     0                       0      1  946857600   \n",
       "3                     2                       2      1  947376000   \n",
       "4                    19                      23      0  948240000   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  This whole series is great way to spend time w...   \n",
       "1                               Entertainingl Funny!   \n",
       "2                                         FANTASTIC!   \n",
       "3                                              GREAT   \n",
       "4    WARNING: CLAMSHELL EDITION IS EDITED TV VERSION   \n",
       "\n",
       "                                                Text  \\\n",
       "0  I can remember seeing the show when it aired o...   \n",
       "1  Beetlejuice is a well written movie ..... ever...   \n",
       "2  Beetlejuice is an excellent and funny movie. K...   \n",
       "3  THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...   \n",
       "4  I, myself always enjoyed this movie, it's very...   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  b'rememb see show air televis year ago child s...  \n",
       "1  b'beetlejuic well written movi everyth excel a...  \n",
       "2  b'beetlejuic excel funni movi keaton hilari wa...  \n",
       "3  b'one movi movi collect fill comedi action wha...  \n",
       "4  b'alway enjoy movi funni entertain didnt hesit...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Total DataSet\n",
    "total_dataset = df.iloc[:total_entries]\n",
    "#total_dataset = df.sample(n=total_entries,random_state=42)\n",
    "print(total_dataset.shape)\n",
    "total_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138683</td>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>b'rememb see show air televis year ago child s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>417839</td>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>b'beetlejuic well written movi everyth excel a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>417838</td>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>b'beetlejuic excel funni movi keaton hilari wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417927</td>\n",
       "      <td>451949</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1048CYU0OV4O8</td>\n",
       "      <td>Judy L. Eans</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>947376000</td>\n",
       "      <td>GREAT</td>\n",
       "      <td>THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...</td>\n",
       "      <td>b'one movi movi collect fill comedi action wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>417847</td>\n",
       "      <td>451864</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1B2IZU1JLZA6</td>\n",
       "      <td>Wes</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>948240000</td>\n",
       "      <td>WARNING: CLAMSHELL EDITION IS EDITED TV VERSION</td>\n",
       "      <td>I, myself always enjoyed this movie, it's very...</td>\n",
       "      <td>b'alway enjoy movi funni entertain didnt hesit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index      Id   ProductId          UserId               ProfileName  \\\n",
       "0  138683  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "1  417839  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "2  417838  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "3  417927  451949  B00004CXX9  A1048CYU0OV4O8              Judy L. Eans   \n",
       "4  417847  451864  B00004CXX9   A1B2IZU1JLZA6                       Wes   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "0                     2                       2      1  940809600   \n",
       "1                     0                       0      1  944092800   \n",
       "2                     0                       0      1  946857600   \n",
       "3                     2                       2      1  947376000   \n",
       "4                    19                      23      0  948240000   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  This whole series is great way to spend time w...   \n",
       "1                               Entertainingl Funny!   \n",
       "2                                         FANTASTIC!   \n",
       "3                                              GREAT   \n",
       "4    WARNING: CLAMSHELL EDITION IS EDITED TV VERSION   \n",
       "\n",
       "                                                Text  \\\n",
       "0  I can remember seeing the show when it aired o...   \n",
       "1  Beetlejuice is a well written movie ..... ever...   \n",
       "2  Beetlejuice is an excellent and funny movie. K...   \n",
       "3  THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...   \n",
       "4  I, myself always enjoyed this movie, it's very...   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  b'rememb see show air televis year ago child s...  \n",
       "1  b'beetlejuic well written movi everyth excel a...  \n",
       "2  b'beetlejuic excel funni movi keaton hilari wa...  \n",
       "3  b'one movi movi collect fill comedi action wha...  \n",
       "4  b'alway enjoy movi funni entertain didnt hesit...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort dataset based on Time\n",
    "total_dataset = total_dataset.sort_values('Time',axis=0, ascending=True, inplace=False, na_position='last')\n",
    "total_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138683</td>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>b'rememb see show air televis year ago child s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>417839</td>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>b'beetlejuic well written movi everyth excel a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>417838</td>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>b'beetlejuic excel funni movi keaton hilari wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417927</td>\n",
       "      <td>451949</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1048CYU0OV4O8</td>\n",
       "      <td>Judy L. Eans</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>947376000</td>\n",
       "      <td>GREAT</td>\n",
       "      <td>THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...</td>\n",
       "      <td>b'one movi movi collect fill comedi action wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>417847</td>\n",
       "      <td>451864</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>A1B2IZU1JLZA6</td>\n",
       "      <td>Wes</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>948240000</td>\n",
       "      <td>WARNING: CLAMSHELL EDITION IS EDITED TV VERSION</td>\n",
       "      <td>I, myself always enjoyed this movie, it's very...</td>\n",
       "      <td>b'alway enjoy movi funni entertain didnt hesit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index      Id   ProductId          UserId               ProfileName  \\\n",
       "0  138683  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "1  417839  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "2  417838  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "3  417927  451949  B00004CXX9  A1048CYU0OV4O8              Judy L. Eans   \n",
       "4  417847  451864  B00004CXX9   A1B2IZU1JLZA6                       Wes   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "0                     2                       2      1  940809600   \n",
       "1                     0                       0      1  944092800   \n",
       "2                     0                       0      1  946857600   \n",
       "3                     2                       2      1  947376000   \n",
       "4                    19                      23      0  948240000   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  This whole series is great way to spend time w...   \n",
       "1                               Entertainingl Funny!   \n",
       "2                                         FANTASTIC!   \n",
       "3                                              GREAT   \n",
       "4    WARNING: CLAMSHELL EDITION IS EDITED TV VERSION   \n",
       "\n",
       "                                                Text  \\\n",
       "0  I can remember seeing the show when it aired o...   \n",
       "1  Beetlejuice is a well written movie ..... ever...   \n",
       "2  Beetlejuice is an excellent and funny movie. K...   \n",
       "3  THIS IS ONE MOVIE THAT SHOULD BE IN YOUR MOVIE...   \n",
       "4  I, myself always enjoyed this movie, it's very...   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  b'rememb see show air televis year ago child s...  \n",
       "1  b'beetlejuic well written movi everyth excel a...  \n",
       "2  b'beetlejuic excel funni movi keaton hilari wa...  \n",
       "3  b'one movi movi collect fill comedi action wha...  \n",
       "4  b'alway enjoy movi funni entertain didnt hesit...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Training Dataset\n",
    "train_dataset = total_dataset.iloc[:n_train_entries]\n",
    "print(train_dataset.shape)\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89923</th>\n",
       "      <td>272268</td>\n",
       "      <td>295091</td>\n",
       "      <td>B002GPG6BE</td>\n",
       "      <td>AVT5YM077UHFE</td>\n",
       "      <td>Vicki R. Boggs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1276560000</td>\n",
       "      <td>Rice Milk is available at Amazon.com!</td>\n",
       "      <td>&lt;a href=\"http://www.amazon.com/gp/product/B002...</td>\n",
       "      <td>b'imagin rice dream drink enrich origin box pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89918</th>\n",
       "      <td>378488</td>\n",
       "      <td>409246</td>\n",
       "      <td>B003750AGE</td>\n",
       "      <td>AQS57P332FTEG</td>\n",
       "      <td>Van Gogh</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1276560000</td>\n",
       "      <td>Grillin beans</td>\n",
       "      <td>These beans lacked flavor and I added a whole ...</td>\n",
       "      <td>b'bean lack flavor ad whole lot ingriedi make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90073</th>\n",
       "      <td>337369</td>\n",
       "      <td>365031</td>\n",
       "      <td>B001AC67DQ</td>\n",
       "      <td>A1270OOH9URZW4</td>\n",
       "      <td>KDragon</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1276646400</td>\n",
       "      <td>Great Marinade</td>\n",
       "      <td>Love the taste! Sweet mesquite. Works great as...</td>\n",
       "      <td>b'love tast sweet mesquit work great marinad b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90074</th>\n",
       "      <td>406009</td>\n",
       "      <td>439058</td>\n",
       "      <td>B000NY31EA</td>\n",
       "      <td>A3OQ3LTW5OTHP1</td>\n",
       "      <td>Peg B.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1276646400</td>\n",
       "      <td>If you could eat memories.....</td>\n",
       "      <td>I used this product to make hamburgers and the...</td>\n",
       "      <td>b'use product make hamburg tast like grandmoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90075</th>\n",
       "      <td>224408</td>\n",
       "      <td>243340</td>\n",
       "      <td>B00305L330</td>\n",
       "      <td>A2B117UIX7AJOA</td>\n",
       "      <td>L. Meaux</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1276646400</td>\n",
       "      <td>Click-tastic!!</td>\n",
       "      <td>So, I am always on the lookout for a good qual...</td>\n",
       "      <td>b'alway lookout good qualiti tasti protein per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index      Id   ProductId          UserId     ProfileName  \\\n",
       "89923  272268  295091  B002GPG6BE   AVT5YM077UHFE  Vicki R. Boggs   \n",
       "89918  378488  409246  B003750AGE   AQS57P332FTEG        Van Gogh   \n",
       "90073  337369  365031  B001AC67DQ  A1270OOH9URZW4         KDragon   \n",
       "90074  406009  439058  B000NY31EA  A3OQ3LTW5OTHP1          Peg B.   \n",
       "90075  224408  243340  B00305L330  A2B117UIX7AJOA        L. Meaux   \n",
       "\n",
       "       HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "89923                     1                       1      1  1276560000   \n",
       "89918                     0                       4      0  1276560000   \n",
       "90073                     1                       1      1  1276646400   \n",
       "90074                     3                       3      1  1276646400   \n",
       "90075                     0                       0      1  1276646400   \n",
       "\n",
       "                                     Summary  \\\n",
       "89923  Rice Milk is available at Amazon.com!   \n",
       "89918                          Grillin beans   \n",
       "90073                         Great Marinade   \n",
       "90074         If you could eat memories.....   \n",
       "90075                         Click-tastic!!   \n",
       "\n",
       "                                                    Text  \\\n",
       "89923  <a href=\"http://www.amazon.com/gp/product/B002...   \n",
       "89918  These beans lacked flavor and I added a whole ...   \n",
       "90073  Love the taste! Sweet mesquite. Works great as...   \n",
       "90074  I used this product to make hamburgers and the...   \n",
       "90075  So, I am always on the lookout for a good qual...   \n",
       "\n",
       "                                             CleanedText  \n",
       "89923  b'imagin rice dream drink enrich origin box pa...  \n",
       "89918  b'bean lack flavor ad whole lot ingriedi make ...  \n",
       "90073  b'love tast sweet mesquit work great marinad b...  \n",
       "90074  b'use product make hamburg tast like grandmoth...  \n",
       "90075  b'alway lookout good qualiti tasti protein per...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Cross Validation Dataset\n",
    "cv_dataset = total_dataset.iloc[n_train_entries:(n_train_entries+n_cv_entries)]\n",
    "print(cv_dataset.shape)\n",
    "cv_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120084</th>\n",
       "      <td>249105</td>\n",
       "      <td>270074</td>\n",
       "      <td>B000LRIFU4</td>\n",
       "      <td>A34RLIHYRNS32O</td>\n",
       "      <td>Eileen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1293926400</td>\n",
       "      <td>best yeast ever</td>\n",
       "      <td>this works. anyone who's ever baked with yeast...</td>\n",
       "      <td>b'work anyon whos ever bake yeast know horror ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120073</th>\n",
       "      <td>394347</td>\n",
       "      <td>426432</td>\n",
       "      <td>B000I62E82</td>\n",
       "      <td>A2X4F1HB3GUKWU</td>\n",
       "      <td>Red Writer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1293926400</td>\n",
       "      <td>Oregon Fruit Boysenberries</td>\n",
       "      <td>I can always count on Oregon Fruit for the bes...</td>\n",
       "      <td>b'alway count oregon fruit best boysenberri pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120004</th>\n",
       "      <td>329113</td>\n",
       "      <td>356190</td>\n",
       "      <td>B0018RYBZ4</td>\n",
       "      <td>A2I3Q8D73940OC</td>\n",
       "      <td>MichiganMommy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1293926400</td>\n",
       "      <td>Doesn't get any better than this</td>\n",
       "      <td>At 13 weeks into my pregnancy I found out I ha...</td>\n",
       "      <td>b'week pregnanc found gestat diabet week sever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119900</th>\n",
       "      <td>438189</td>\n",
       "      <td>473861</td>\n",
       "      <td>B000FPDYR6</td>\n",
       "      <td>A2N9HNTRR1ZDZG</td>\n",
       "      <td>SV \"TeaRific\"</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1293926400</td>\n",
       "      <td>words are not enough - groans of pleasure best...</td>\n",
       "      <td>A checker snorted at seeing this product. I to...</td>\n",
       "      <td>b'checker snort see product told allergi often...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120002</th>\n",
       "      <td>417520</td>\n",
       "      <td>451524</td>\n",
       "      <td>B001M0503E</td>\n",
       "      <td>A16D04BXWBATFS</td>\n",
       "      <td>Jack Jericho</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1293926400</td>\n",
       "      <td>Delicious</td>\n",
       "      <td>I'm a strawberry lover, I'll eat anything stra...</td>\n",
       "      <td>b'strawberri lover ill eat anyth strawberri de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index      Id   ProductId          UserId    ProfileName  \\\n",
       "120084  249105  270074  B000LRIFU4  A34RLIHYRNS32O         Eileen   \n",
       "120073  394347  426432  B000I62E82  A2X4F1HB3GUKWU     Red Writer   \n",
       "120004  329113  356190  B0018RYBZ4  A2I3Q8D73940OC  MichiganMommy   \n",
       "119900  438189  473861  B000FPDYR6  A2N9HNTRR1ZDZG  SV \"TeaRific\"   \n",
       "120002  417520  451524  B001M0503E  A16D04BXWBATFS   Jack Jericho   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "120084                     1                       1      1  1293926400   \n",
       "120073                     0                       0      1  1293926400   \n",
       "120004                     0                       0      1  1293926400   \n",
       "119900                     0                       1      1  1293926400   \n",
       "120002                     0                       0      1  1293926400   \n",
       "\n",
       "                                                  Summary  \\\n",
       "120084                                    best yeast ever   \n",
       "120073                         Oregon Fruit Boysenberries   \n",
       "120004                   Doesn't get any better than this   \n",
       "119900  words are not enough - groans of pleasure best...   \n",
       "120002                                          Delicious   \n",
       "\n",
       "                                                     Text  \\\n",
       "120084  this works. anyone who's ever baked with yeast...   \n",
       "120073  I can always count on Oregon Fruit for the bes...   \n",
       "120004  At 13 weeks into my pregnancy I found out I ha...   \n",
       "119900  A checker snorted at seeing this product. I to...   \n",
       "120002  I'm a strawberry lover, I'll eat anything stra...   \n",
       "\n",
       "                                              CleanedText  \n",
       "120084  b'work anyon whos ever bake yeast know horror ...  \n",
       "120073  b'alway count oregon fruit best boysenberri pi...  \n",
       "120004  b'week pregnanc found gestat diabet week sever...  \n",
       "119900  b'checker snort see product told allergi often...  \n",
       "120002  b'strawberri lover ill eat anyth strawberri de...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Test Dataset\n",
    "test_dataset = total_dataset.iloc[(n_train_entries+n_cv_entries):]\n",
    "print(test_dataset.shape)\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------------+------------------+---------------+\n",
      "| DataSet | Total | Postivie Reviews | Negative Reviews |     Ratio     |\n",
      "+---------+-------+------------------+------------------+---------------+\n",
      "|  Train  | 90000 |      78845       |      11155       | 0.876 : 0.124 |\n",
      "|    CV   | 30000 |      25062       |       4938       | 0.835 : 0.165 |\n",
      "|   Test  | 30000 |      25020       |       4980       | 0.834 : 0.166 |\n",
      "+---------+-------+------------------+------------------+---------------+\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What is balance of review polarity in our dataset(s)?\n",
    "class_ratio = PrettyTable()\n",
    "class_ratio.field_names = [\"DataSet\", \"Total\", \"Postivie Reviews\", \"Negative Reviews\", \"Ratio\"]\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "pos_rev_count = train_dataset[train_dataset.Score == 1].Score.value_counts().tolist()[0]\n",
    "neg_rev_count = train_dataset[train_dataset.Score == 0].Score.value_counts().tolist()[0]\n",
    "ratio = '{0} : {1}'.format(round(pos_rev_count/len(train_dataset),3),round(neg_rev_count/len(train_dataset),3))\n",
    "class_ratio.add_row(['Train', len(train_dataset), pos_rev_count, neg_rev_count, ratio])\n",
    "plt.subplot(1,3,1)\n",
    "plt.pie([neg_rev_count, pos_rev_count], labels=['-ive', '+ive']);\n",
    "plt.title('Training Dataset');\n",
    "\n",
    "pos_rev_count = cv_dataset[cv_dataset.Score == 1].Score.value_counts().tolist()[0]\n",
    "neg_rev_count = cv_dataset[cv_dataset.Score == 0].Score.value_counts().tolist()[0]\n",
    "ratio = '{0} : {1}'.format(round(pos_rev_count/len(cv_dataset),3),round(neg_rev_count/len(cv_dataset),3))\n",
    "class_ratio.add_row(['CV', len(cv_dataset), pos_rev_count, neg_rev_count, ratio])\n",
    "plt.subplot(1,3,2)\n",
    "plt.pie([neg_rev_count, pos_rev_count], labels=['-ive', '+ive']);\n",
    "plt.title('CV Dataset');\n",
    "\n",
    "pos_rev_count = test_dataset[test_dataset.Score == 1].Score.value_counts().tolist()[0]\n",
    "neg_rev_count = test_dataset[test_dataset.Score == 0].Score.value_counts().tolist()[0]\n",
    "ratio = '{0} : {1}'.format(round(pos_rev_count/len(test_dataset),3),round(neg_rev_count/len(test_dataset),3))\n",
    "class_ratio.add_row(['Test', len(test_dataset), pos_rev_count, neg_rev_count, ratio])\n",
    "plt.subplot(1,3,3)\n",
    "plt.pie([neg_rev_count, pos_rev_count], labels=['-ive', '+ive']);\n",
    "plt.title('Test Dataset');\n",
    "\n",
    "print(class_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Sparse Matrix\n",
    "\n",
    "- Create BoW Vectorizer Model with:\n",
    "    - min word frequency of 30\n",
    "    - considering both uni-gram and bi-gram words\n",
    "    - of maximum 3000 features\n",
    "- Fit Training Data to the BoW Model\n",
    "- For each dataset (Training, Cross-Validataion and Test)\n",
    "    - Transform DataSet using the trained model\n",
    "    - Store the Transformed data\n",
    "    - Row Normalize the Training Data\n",
    "    - Store the Row Normalized data in a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genBowDataSets():\n",
    "    '''\n",
    "    Generates BoW Vectors for Training, CV and Test datasets\n",
    "    '''\n",
    "    #################################\n",
    "    # Vectorizing  Training Dataset #\n",
    "    #################################\n",
    "    print('Generating Training Dataset')\n",
    "    data_array = train_dataset['CleanedText']\n",
    "    label_array = train_dataset['Score']\n",
    "    file_prefix = 'Train'\n",
    "\n",
    "    label_file_name = getOutputFileNamePath('{0}_bow_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_bow'.format(file_prefix))\n",
    "    vec_svd_file_name = getOutputFileNamePath('{0}_bow_svd'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_bow_std'.format(file_prefix))\n",
    "\n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array))\n",
    "\n",
    "    # Train the Model\n",
    "    bow_model = CountVectorizer(min_df=30, max_features=2500, ngram_range=(1,2))\n",
    "    bow_vectorizer = bow_model.fit(data_array)\n",
    "\n",
    "    bow_vector = bow_vectorizer.transform(data_array)\n",
    "    print('Shape of BoW Vectorizer: ', bow_vector.get_shape())\n",
    "    print('Total no.of unique words: ', bow_vector.get_shape()[1])\n",
    "    scipy.sparse.save_npz(vec_file_name, scipy.sparse.csr_matrix(bow_vector))\n",
    "    \n",
    "    # Reduce dimensionality using TruncatedSVD\n",
    "    max_svd_components = GetOptimalDimension(bow_vector)\n",
    "    svd_model = GetRandomizedSVD(n_components=max_svd_components)\n",
    "    svd_model = svd_model.fit(bow_vector)\n",
    "    svd_data = svd_model.transform(bow_vector)\n",
    "    scipy.sparse.save_npz(vec_svd_file_name, scipy.sparse.csr_matrix(svd_data))\n",
    "\n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = bow_vector.todense().astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))\n",
    "    \n",
    "    ##########################\n",
    "    # Vectorizing CV Dataset #\n",
    "    ##########################\n",
    "    print('\\nGenerating CV Dataset')\n",
    "    data_array = cv_dataset['CleanedText']\n",
    "    label_array = cv_dataset['Score']\n",
    "    file_prefix = 'CV'\n",
    "\n",
    "    label_file_name = getOutputFileNamePath('{0}_bow_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_bow'.format(file_prefix))\n",
    "    vec_svd_file_name = getOutputFileNamePath('{0}_bow_svd'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_bow_std'.format(file_prefix))\n",
    "\n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array))\n",
    "\n",
    "    # Transform the Dataset using fitted model\n",
    "    bow_vector = bow_vectorizer.transform(data_array)\n",
    "    print('Shape of BoW Vectorizer: ', bow_vector.get_shape())\n",
    "    print('Total no.of unique words: ', bow_vector.get_shape()[1])\n",
    "    scipy.sparse.save_npz(vec_file_name, scipy.sparse.csr_matrix(bow_vector))\n",
    "    \n",
    "    # Reduce dimensionality using TruncatedSVD\n",
    "    svd_data = svd_model.transform(bow_vector)\n",
    "    scipy.sparse.save_npz(vec_svd_file_name, scipy.sparse.csr_matrix(svd_data))\n",
    "\n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = bow_vector.todense().astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))\n",
    "\n",
    "    \n",
    "    ############################\n",
    "    # Vectorizing Test Dataset #\n",
    "    ############################\n",
    "    print('\\nGenerating Test Dataset')\n",
    "    data_array = test_dataset['CleanedText']\n",
    "    label_array = test_dataset['Score']\n",
    "    file_prefix = 'Test'\n",
    "\n",
    "    label_file_name = getOutputFileNamePath('{0}_bow_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_bow'.format(file_prefix))\n",
    "    vec_svd_file_name = getOutputFileNamePath('{0}_bow_svd'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_bow_std'.format(file_prefix))\n",
    "\n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array))\n",
    "\n",
    "    # Transform the Dataset using fitted model\n",
    "    bow_vector = bow_vectorizer.transform(data_array)\n",
    "    print('Shape of BoW Vectorizer: ', bow_vector.get_shape())\n",
    "    print('Total no.of unique words: ', bow_vector.get_shape()[1])\n",
    "    scipy.sparse.save_npz(vec_file_name, scipy.sparse.csr_matrix(bow_vector))\n",
    "    \n",
    "    # Reduce dimensionality using TruncatedSVD\n",
    "    svd_data = svd_model.transform(bow_vector)\n",
    "    scipy.sparse.save_npz(vec_svd_file_name, scipy.sparse.csr_matrix(svd_data))\n",
    "\n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = bow_vector.todense().astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Training Dataset\n",
      "Shape of BoW Vectorizer:  (90000, 2500)\n",
      "Total no.of unique words:  2500\n",
      "GetOptimalDimension -> Actual:  2500  Selected:  2499\n",
      "GetOptimalDimension -> Final SVD Component Size:  1537\n",
      "Shape of Scaled data (90000, 2500)\n",
      "\n",
      "Generating CV Dataset\n",
      "Shape of BoW Vectorizer:  (30000, 2500)\n",
      "Total no.of unique words:  2500\n",
      "Shape of Scaled data (30000, 2500)\n",
      "\n",
      "Generating Test Dataset\n",
      "Shape of BoW Vectorizer:  (30000, 2500)\n",
      "Total no.of unique words:  2500\n",
      "Shape of Scaled data (30000, 2500)\n",
      "Wall time: 6min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "genBowDataSets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTfIdfDataSets():\n",
    "    '''\n",
    "    Generates TF-IDF Vectors for Training, CV and Test datasets\n",
    "    '''\n",
    "    #################################\n",
    "    # Vectorizing  Training Dataset #\n",
    "    #################################\n",
    "    print('Generating Training Dataset')\n",
    "    data_array = train_dataset['CleanedText']\n",
    "    label_array = train_dataset['Score']\n",
    "    file_prefix = 'Train'\n",
    "\n",
    "    label_file_name = getOutputFileNamePath('{0}_tfidf_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_tfidf'.format(file_prefix))\n",
    "    vec_svd_file_name = getOutputFileNamePath('{0}_tfid_svd'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_tfidf_std'.format(file_prefix))\n",
    "\n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array))\n",
    "\n",
    "    # Train the Model\n",
    "    tfidf_model = TfidfVectorizer(min_df=30, max_features=2000, ngram_range=(1,2))\n",
    "    tfidf_vectorizer = tfidf_model.fit(data_array)\n",
    "\n",
    "    tfidf_vector = tfidf_vectorizer.transform(data_array)\n",
    "    print('Shape of TfIDF Vectorizer: ', tfidf_vector.get_shape())\n",
    "    print('Total no.of unique words: ', tfidf_vector.get_shape()[1])\n",
    "    scipy.sparse.save_npz(vec_file_name, scipy.sparse.csr_matrix(tfidf_vector))\n",
    "\n",
    "    \n",
    "    # Reduce dimensionality using TruncatedSVD\n",
    "    max_svd_components = GetOptimalDimension(tfidf_vector)\n",
    "    svd_model = GetRandomizedSVD(n_components=max_svd_components)\n",
    "    svd_model = svd_model.fit(tfidf_vector)\n",
    "    svd_data = svd_model.transform(tfidf_vector)\n",
    "    scipy.sparse.save_npz(vec_svd_file_name, scipy.sparse.csr_matrix(svd_data))\n",
    "\n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = tfidf_vector.todense().astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))\n",
    "    \n",
    "    ##########################\n",
    "    # Vectorizing CV Dataset #\n",
    "    ##########################\n",
    "    print('\\nGenerating CV Dataset')\n",
    "    data_array = cv_dataset['CleanedText']\n",
    "    label_array = cv_dataset['Score']\n",
    "    file_prefix = 'CV'\n",
    "\n",
    "    label_file_name = getOutputFileNamePath('{0}_tfidf_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_tfidf'.format(file_prefix))\n",
    "    vec_svd_file_name = getOutputFileNamePath('{0}_tfid_svd'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_tfidf_std'.format(file_prefix))\n",
    "\n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array))\n",
    "\n",
    "    # Transform the Dataset using fitted model\n",
    "    tfidf_vector = tfidf_vectorizer.transform(data_array)\n",
    "    print('Shape of TfIDF Vectorizer: ', tfidf_vector.get_shape())\n",
    "    print('Total no.of unique words: ', tfidf_vector.get_shape()[1])\n",
    "    scipy.sparse.save_npz(vec_file_name, scipy.sparse.csr_matrix(tfidf_vector))\n",
    "\n",
    "    # Reduce dimensionality using TruncatedSVD\n",
    "    svd_data = svd_model.transform(tfidf_vector)\n",
    "    scipy.sparse.save_npz(vec_svd_file_name, scipy.sparse.csr_matrix(svd_data))\n",
    "\n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = tfidf_vector.todense().astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))\n",
    "    \n",
    "    ############################\n",
    "    # Vectorizing Test Dataset #\n",
    "    ############################\n",
    "    print('\\nGenerating Test Dataset')\n",
    "    data_array = test_dataset['CleanedText']\n",
    "    label_array = test_dataset['Score']\n",
    "    file_prefix = 'Test'\n",
    "\n",
    "    label_file_name = getOutputFileNamePath('{0}_tfidf_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_tfidf'.format(file_prefix))\n",
    "    vec_svd_file_name = getOutputFileNamePath('{0}_tfid_svd'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_tfidf_std'.format(file_prefix))\n",
    "\n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array))\n",
    "\n",
    "    # Transform the Dataset using fitted model\n",
    "    tfidf_vector = tfidf_vectorizer.transform(data_array)\n",
    "    print('Shape of BoW Vectorizer: ', tfidf_vector.get_shape())\n",
    "    print('Total no.of unique words: ', tfidf_vector.get_shape()[1])\n",
    "    scipy.sparse.save_npz(vec_file_name, scipy.sparse.csr_matrix(tfidf_vector))\n",
    "\n",
    "    # Reduce dimensionality using TruncatedSVD\n",
    "    svd_data = svd_model.transform(tfidf_vector)\n",
    "    scipy.sparse.save_npz(vec_svd_file_name, scipy.sparse.csr_matrix(svd_data))\n",
    "\n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = tfidf_vector.todense().astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Training Dataset\n",
      "Shape of TfIDF Vectorizer:  (90000, 2000)\n",
      "Total no.of unique words:  2000\n",
      "GetOptimalDimension -> Actual:  2000  Selected:  1999\n",
      "GetOptimalDimension -> Final SVD Component Size:  1606\n",
      "Shape of Scaled data (90000, 2000)\n",
      "\n",
      "Generating CV Dataset\n",
      "Shape of TfIDF Vectorizer:  (30000, 2000)\n",
      "Total no.of unique words:  2000\n",
      "Shape of Scaled data (30000, 2000)\n",
      "\n",
      "Generating Test Dataset\n",
      "Shape of BoW Vectorizer:  (30000, 2000)\n",
      "Total no.of unique words:  2000\n",
      "Shape of Scaled data (30000, 2000)\n",
      "Wall time: 5min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "genTfIdfDataSets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Word2Vec\n",
    "\n",
    "Max Dimension for Word2Vec = 50 (some arbitraty value, no calculation made on selecting this value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfSentences(data_array):\n",
    "    '''\n",
    "    returns list having each sentence as an entry\n",
    "    '''\n",
    "    list_of_sent = []\n",
    "    for sent in data_array:\n",
    "        list_of_sent.append(sent.decode(\"utf-8\").split())\n",
    "    return list_of_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgW2VReviewVectors(w2v_model,data_array):\n",
    "    '''\n",
    "    returns the w2v for all the reviews that exist in data_array using w2v_vocab\n",
    "    Input:\n",
    "        w2v_model - Model which need to be used for vectorization\n",
    "        data_array - Reviews that need to be vectorized\n",
    "    Output:\n",
    "        List having W2V Vectorized data for data_array\n",
    "    '''\n",
    "    list_of_sent = getListOfSentences(data_array)\n",
    "    w2v_words = list(w2v_model.wv.vocab)\n",
    "    #saveListToFile(file_prefix + '_avg_w2v_w2v_words',w2v_words)\n",
    "    #print(\"number of words that occured minimum 5 times : \",len(w2v_words))\n",
    "    #print(\"sample words \", w2v_words[0:50])\n",
    "\n",
    "    # Computing average w2v for each review in selected training dataset\n",
    "    review_vectors = []\n",
    "    for sent in tqdm(list_of_sent, ascii=True):\n",
    "        sent_vec = np.zeros(w2v_d) # array to hold the vectors. Initially assuming no vectors in this review\n",
    "        no_of_words_in_review = 0 # number of words with valid vector in this review\n",
    "\n",
    "        # count all the words (that are in w2v model) and take average\n",
    "        for word in sent:\n",
    "            if word in w2v_words:\n",
    "                vec = w2v_model.wv[word]\n",
    "                sent_vec += vec\n",
    "                no_of_words_in_review += 1\n",
    "        if no_of_words_in_review != 0:\n",
    "            sent_vec /= no_of_words_in_review\n",
    "        review_vectors.append(sent_vec)\n",
    "        \n",
    "    return review_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required dimension\n",
    "w2v_d = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgW2VDataSet():\n",
    "    '''\n",
    "    Generates Average Word2Vec Vector for Training, CV and Test datasets\n",
    "    '''\n",
    "    #################################\n",
    "    # Vectorizing  Training Dataset #\n",
    "    #################################\n",
    "    print('Generating Training Dataset')\n",
    "    data_array = train_dataset['CleanedText']\n",
    "    label_array = train_dataset['Score']\n",
    "    file_prefix = 'Train'\n",
    "\n",
    "    label_file_name = getOutputFileNamePath('{0}_avg_w2v_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_avg_w2v'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_avg_w2v_std'.format(file_prefix))\n",
    "    \n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array.T))\n",
    "    \n",
    "    # Create Training DataSet List array for creating own W2V\n",
    "    train_list_of_sent = getListOfSentences(data_array.values)\n",
    "        \n",
    "    # Considering words that are occured atleast 5 times in the corpus\n",
    "    w2v_model = Word2Vec(train_list_of_sent, min_count=5, size=w2v_d, workers=16)\n",
    "\n",
    "    # Computing average w2v for each review in selected training dataset\n",
    "    review_vectors = getAvgW2VReviewVectors(w2v_model, data_array)\n",
    "    np.save(vec_file_name, review_vectors)\n",
    "    #print(len(review_vectors))\n",
    "    #print(len(review_vectors[0]))\n",
    "    \n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = np.array(review_vectors).astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))\n",
    "    \n",
    "    ##########################\n",
    "    # Vectorizing CV Dataset #\n",
    "    ##########################\n",
    "    print('\\nGenerating CV Dataset')\n",
    "    data_array = cv_dataset['CleanedText']\n",
    "    label_array = cv_dataset['Score']\n",
    "    file_prefix = 'CV'\n",
    "    \n",
    "    label_file_name = getOutputFileNamePath('{0}_avg_w2v_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_avg_w2v'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_avg_w2v_std'.format(file_prefix))\n",
    "    \n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array.T))\n",
    "    \n",
    "    # Computing average w2v for each review in selected training dataset\n",
    "    review_vectors = getAvgW2VReviewVectors(w2v_model, data_array)\n",
    "    np.save(vec_file_name, review_vectors)\n",
    "    #print(len(review_vectors))\n",
    "    #print(len(review_vectors[0]))\n",
    "    \n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = np.array(review_vectors).astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))\n",
    "    \n",
    "    ############################\n",
    "    # Vectorizing Test Dataset #\n",
    "    ############################\n",
    "    print('\\nGenerating Test Dataset')\n",
    "    data_array = test_dataset['CleanedText']\n",
    "    label_array = test_dataset['Score']\n",
    "    file_prefix = 'Test'\n",
    "    \n",
    "    label_file_name = getOutputFileNamePath('{0}_avg_w2v_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_avg_w2v'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_avg_w2v_std'.format(file_prefix))\n",
    "    \n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array.T))\n",
    "    \n",
    "    # Computing average w2v for each review in selected training dataset\n",
    "    review_vectors = getAvgW2VReviewVectors(w2v_model, data_array)\n",
    "    np.save(vec_file_name, review_vectors)\n",
    "    #print(len(review_vectors))\n",
    "    #print(len(review_vectors[0]))\n",
    "    \n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = np.array(review_vectors).astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Training Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########################################################################| 90000/90000 [01:20<00:00, 1111.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Scaled data (90000, 50)\n",
      "\n",
      "Generating CV Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###########################################################################| 30000/30000 [00:30<00:00, 986.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Scaled data (30000, 50)\n",
      "\n",
      "Generating Test Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###########################################################################| 30000/30000 [00:33<00:00, 902.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Scaled data (30000, 50)\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "getAvgW2VDataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## TF-IDF Weighted Word2Vec\n",
    "\n",
    "Max Dimension for Word2Vec = 50 (some arbitraty value, no calculation made on selecting this value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDFWW2VReviewVectors(w2v_model,tf_idf_dict,data_array):\n",
    "    '''\n",
    "    returns the w2v for all the reviews that exist in data_array using w2v_vocab\n",
    "    Input:\n",
    "        w2v_model - Model which need to be used for vectorization\n",
    "        tf_idf_dict - Dictionary having the vocabularies\n",
    "        data_array - Reviews that need to be vectorized\n",
    "    Output:\n",
    "        List having W2V Vectorized data for data_array\n",
    "    '''\n",
    "    list_of_sent = getListOfSentences(data_array)\n",
    "    w2v_words = list(w2v_model.wv.vocab)\n",
    "    \n",
    "    review_vectors = []\n",
    "    for sent in tqdm(list_of_sent, ascii=True):\n",
    "        sent_vec = np.zeros(w2v_d) # array to hold the vectors\n",
    "        no_of_words_in_review = 0 # number of words with valid vector in this review\n",
    "\n",
    "        # count all the words (that are in w2v model) and take average\n",
    "        for word in sent:\n",
    "            if word in w2v_words:\n",
    "                vec = w2v_model.wv[word]\n",
    "                # calculate tf-idf weighted w2v value for this word\n",
    "                tf_idf = tf_idf_dict[word] * (sent.count(word)/len(sent))\n",
    "                sent_vec += (vec * tf_idf)\n",
    "                no_of_words_in_review += 1\n",
    "        if no_of_words_in_review != 0:\n",
    "            sent_vec /= no_of_words_in_review\n",
    "        review_vectors.append(sent_vec)\n",
    "    \n",
    "    return review_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTfIdfWeightedW2VDataset():\n",
    "    '''\n",
    "    Generates Average Word2Vec Vector for Training, CV and Test datasets\n",
    "    '''\n",
    "    #################################\n",
    "    # Vectorizing  Training Dataset #\n",
    "    #################################\n",
    "    print('Generating Training Dataset')\n",
    "    data_array = train_dataset['CleanedText']\n",
    "    label_array = train_dataset['Score']\n",
    "    file_prefix = 'Train'\n",
    "\n",
    "    label_file_name = getOutputFileNamePath('{0}_tfidf_w_w2v_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_tfidf_w_w2v'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_tfidf_w_w2v_std'.format(file_prefix))\n",
    "    \n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array.T))\n",
    "    \n",
    "    # Create List array for creating own W2V\n",
    "    train_list_of_sent = getListOfSentences(data_array.values)\n",
    "\n",
    "    # Create tf-idf vector matrix\n",
    "    tf_idf_model = TfidfVectorizer(ngram_range=(1,2))\n",
    "    tf_idf_matrix = tf_idf_model.fit_transform(data_array.values)\n",
    "\n",
    "    # Create dictionary having words (features) as keys, its tf-idf values as values\n",
    "    tf_idf_dict = dict(zip(tf_idf_model.get_feature_names(), list(tf_idf_model.idf_)))\n",
    "    len(tf_idf_dict)\n",
    "    \n",
    "    tf_idf_feat = tf_idf_model.get_feature_names()\n",
    "    \n",
    "    # Considering words that are occured atleast 5 times in the corpus\n",
    "    w2v_model = Word2Vec(train_list_of_sent, min_count=5, size=w2v_d, workers=16)\n",
    "\n",
    "    w2v_words = list(w2v_model.wv.vocab)\n",
    "    #print(\"number of words that occured minimum 5 times : \",len(w2v_words))\n",
    "    #print(\"sample words \", w2v_words[0:50])\n",
    "\n",
    "    # Computing tf-idf weighted w2v for each review in selected training dataset\n",
    "    review_vectors = getTFIDFWW2VReviewVectors(w2v_model, tf_idf_dict, data_array)\n",
    "    np.save(vec_file_name, review_vectors)\n",
    "    #print(len(review_vectors))\n",
    "    #print(len(review_vectors[0]))\n",
    "    \n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = np.array(review_vectors).astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))\n",
    "    \n",
    "    ##########################\n",
    "    # Vectorizing CV Dataset #\n",
    "    ##########################\n",
    "    print('\\nGenerating CV Dataset')\n",
    "    data_array = cv_dataset['CleanedText']\n",
    "    label_array = cv_dataset['Score']\n",
    "    file_prefix = 'CV'\n",
    "    \n",
    "    label_file_name = getOutputFileNamePath('{0}_tfidf_w_w2v_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_tfidf_w_w2v'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_tfidf_w_w2v_std'.format(file_prefix))\n",
    "    \n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array.T))\n",
    "    \n",
    "    # Computing tf-idf weighted w2v for each review in selected training dataset\n",
    "    review_vectors = getTFIDFWW2VReviewVectors(w2v_model, tf_idf_dict, data_array)\n",
    "    np.save(vec_file_name, review_vectors)\n",
    "    \n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = np.array(review_vectors).astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))\n",
    "    \n",
    "    ############################\n",
    "    # Vectorizing Test Dataset #\n",
    "    ############################\n",
    "    print('\\nGenerating Test Dataset')\n",
    "    data_array = test_dataset['CleanedText']\n",
    "    label_array = test_dataset['Score']\n",
    "    file_prefix = 'Test'\n",
    "    \n",
    "    label_file_name = getOutputFileNamePath('{0}_tfidf_w_w2v_label'.format(file_prefix))\n",
    "    vec_file_name = getOutputFileNamePath('{0}_tfidf_w_w2v'.format(file_prefix))\n",
    "    ved_std_file_name = getOutputFileNamePath('{0}_tfidf_w_w2v_std'.format(file_prefix))\n",
    "    \n",
    "    # Store Label as a separate file\n",
    "    scipy.sparse.save_npz(label_file_name, scipy.sparse.csr_matrix(label_array.T))\n",
    "    \n",
    "    # Computing tf-idf weighted w2v for each review in selected training dataset\n",
    "    review_vectors = getTFIDFWW2VReviewVectors(w2v_model, tf_idf_dict, data_array)\n",
    "    np.save(vec_file_name, review_vectors)\n",
    "    \n",
    "    # Row-Normalize the Data\n",
    "    scaler_model = MinMaxScaler()\n",
    "    scaler_data = np.array(review_vectors).astype(np.float64).T\n",
    "    scaler_model = scaler_model.fit(scaler_data)\n",
    "    scaler_data = scaler_model.transform(scaler_data)\n",
    "    scaler_data = scaler_data.T\n",
    "    print('Shape of Scaled data', scaler_data.shape)\n",
    "    scipy.sparse.save_npz(ved_std_file_name, scipy.sparse.csr_matrix(scaler_data))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Training Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###########################################################################| 90000/90000 [02:17<00:00, 652.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Scaled data (90000, 50)\n",
      "\n",
      "Generating CV Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###########################################################################| 30000/30000 [00:51<00:00, 578.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Scaled data (30000, 50)\n",
      "\n",
      "Generating Test Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###########################################################################| 30000/30000 [01:08<00:00, 437.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Scaled data (30000, 50)\n",
      "Wall time: 4min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "getTfIdfWeightedW2VDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Data-Cleaning-AFF-Review.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
